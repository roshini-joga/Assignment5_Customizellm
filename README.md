# Assignment5_Customizellm

# ğŸ§  LLM Fine-Tuning & Reward Modeling Assignment using Unsloth

## ğŸ”— Submission by: [Your Name]  
## ğŸ“… Date: [Submission Date]

---

## ğŸ“ Contents

This repository contains Colab notebooks and demonstration videos for the following tasks using the **Unsloth AI** library:

- A) Fine-tuning various open-weight LLMs on unique use cases  
- B) Continued Pretraining (Language Learning)  
- C) Chat Templates for multiple tasks  
- D) Reward Modeling using ORPO & DPO  
- E) Continued Fine-Tuning from Custom Checkpoints  
- F) Mental Health Chatbot using Phi-3  
- G) Ollama Export + Inference  
- âœ… GitHub Repo with documentation (this file)

---

## âœ… Submission Guidelines

Each section contains:
- ğŸ”— A **Google Colab notebook link** (with outputs)
- ğŸ¥ A **video walkthrough link** (explaining the process and results)

---

## ğŸ§© A) Fine-Tuning LLMs on Unique Use Cases

| Model Used       | Use Case            | Colab Notebook | Video Link |
|------------------|---------------------|----------------|------------|
| LLaMA 3.1 (8B)    | Chatbot              | [Colab](#)     | [Video](#) |
| Mistral NeMo     | Coding Assistant     | [Colab](#)     | [Video](#) |
| Gemma 2 (9B)     | Text Classification  | [Colab](#)     | [Video](#) |
| TinyLlama        | Long Context Chat    | [Colab](#)     | [Video](#) |

> *Note: Each notebook uses LoRA + Unsloth-based fine-tuning and logs loss, metrics, and inference validation.*

---

## ğŸ§  B) Continued Pretraining

- ğŸ§¾ **Task:** Make LLM learn a new language  
- ğŸ“˜ **Language Used:** [Specify Language]  
- ğŸ”— [Colab Notebook](#)  
- ğŸ¥ [Video Walkthrough](#)

---

## ğŸ’¬ C) Chat Templates Use Cases

| Task Type                      | Colab Notebook | Video Link |
|-------------------------------|----------------|------------|
| Conversational Chatbot        | [Colab](#)     | [Video](#) |
| Classification via Templates  | [Colab](#)     | [Video](#) |
| Extended Context (TinyLlama)  | [Colab](#)     | [Video](#) |
| Multi-Dataset Single Training | [Colab](#)     | [Video](#) |

---

## ğŸ¯ D) Reward Modeling - DPO & ORPO

| Reward Method | Colab Notebook | Video Link |
|---------------|----------------|------------|
| DPO (Direct Preference) | [Colab](#) | [Video](#) |
| ORPO (Optimal Reward)   | [Colab](#) | [Video](#) |

> *Each notebook uses preference-based data and logs accuracy/reward shift.*

---

## ğŸ§© E) Continued Fine-tuning from Custom Checkpoint

- ğŸ”— [Colab Notebook](#)  
- ğŸ¥ [Video Walkthrough](#)

---

## ğŸ§˜â€â™€ï¸ F) Mental Health Chatbot using Phi-3

- ğŸ”— [Colab Notebook](#)  
- ğŸ¥ [Video Walkthrough](#)

> *Model is trained on mental health-specific conversations and validated via chat.*

---

## ğŸ” G) Export to Ollama & Inference

- ğŸ”— [Colab Notebook](#)  
- ğŸ”— [Model exported to Ollama](#)  
- ğŸ§ª **Local Inference Demo**: [Video Link](#)

---

## ğŸ“˜ GitHub Repo Structure

