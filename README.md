# Assignment5_Customizellm

# LLM Fine-Tuning & Reward Modeling Assignment using Unsloth
---

## 📁 Contents

This repository contains Colab notebooks and demonstration videos for the following tasks using the **Unsloth AI** library:

- A) Fine-tuning various open-weight LLMs on unique use cases  
- B) Continued Pretraining (Language Learning)  
- C) Chat Templates for multiple tasks  
- D) Reward Modeling using ORPO & DPO  
- E) Continued Fine-Tuning from Custom Checkpoints  
- F) Mental Health Chatbot using Phi-3  
- G) Ollama Export + Inference  
---

## 🧩 A) Fine-Tuning LLMs on Unique Use Cases

| Model Used       | Use Case            | Colab Notebook | Video Link |
|------------------|---------------------|----------------|------------|
| LLaMA 3.1 (8B)   | Coding Assistant    | [Colab](https://colab.research.google.com/drive/1puHytPYl48lsiUywdddyeEbgI78azglt)     | [Video](https://youtu.be/g1GIxGEVRa0) |
| Mistral NeMo     | Conversational chat  | [Colab](https://colab.research.google.com/drive/1FPYlz8vdaGCHA-6vipkUzJDZQB9Bxe17#scrollTo=05ae6956)     | [Video](https://youtu.be/o-4VjLK0AFw) |
| Gemma 2 (9B)     | Text Summarization   | [Colab](https://colab.research.google.com/drive/1X3nYiYiZjdCMgBQ7SJGCaZeLVUqOwtz0)     | [Video](https://youtu.be/Lr6QMYuhTSI) |
| Phi3.5           | Sentiment Analysis   | [Colab](https://colab.research.google.com/drive/181ccUJMnpajYeQLkEpnB_ZSSfotSEm0A)     | [Video](https://youtu.be/-kF96GxwEfY) |

> *Note: Each notebook uses LoRA + Unsloth-based fine-tuning and logs loss, metrics, and inference validation.*

---

##  B) Continued Pretraining

- 🧾 **Task:** Make LLM learn a new language  
- 📘 **Language Used:** [Telugu]  
- 🔗 [Colab Notebook](https://colab.research.google.com/drive/1O-7G-kVvxNe1JUVLIc64Ejshv8kXdgRN#scrollTo=Ku1qNpnREwuM)  
- 🎥 [Video Walkthrough](https://youtu.be/AUyo8XFVEi4)

---

##  C) Chat Templates Use Cases

Conversational Chatbot,Classification via Templates,Extended Context (TinyLlama),Multi-Dataset Single Training   
| [Colab](https://colab.research.google.com/drive/1eHOZ3Cj1iPUanWyjcrl7m0LId4OB5UT9#scrollTo=W4mkSncb4ffZ) |   
| [Video]() |

## 🎯 D) Reward Modeling - DPO & ORPO
| DPO (Direct Preference),ORPO (Optimal Reward)| [Colab](https://colab.research.google.com/drive/1Au1iYaK3KwRzLphHnGqMJCMge-1q9pkW#scrollTo=CuIoybYmAiJE) | [Video]() |

> *Each notebook uses preference-based data and logs accuracy/reward shift.*

---

## 🧩 E) Continued Fine-tuning from Custom Checkpoint

- 🔗 [Colab Notebook]()  
- 🎥 [Video Walkthrough](#)

---

## 🧘‍♀️ F) Mental Health Chatbot using Phi-3

- 🔗 [Colab Notebook](https://colab.research.google.com/drive/1FpjjmWD3-HqQfWZQZaTEx4T0lvYJlBxs?usp=sharing)  
- 🎥 [Video Walkthrough](#)

> *Model is trained on mental health-specific conversations and validated via chat.*

---

## 🔁 G) Export to Ollama & Inference

- 🔗 [Colab Notebook](https://colab.research.google.com/drive/1Zdtd7VFTsho4WJESpELa8UoZqxDD2nIT#scrollTo=pCqnaKmlO1U9)   
- 🧪 **Demo**: [Video Link](https://youtu.be/IGkEYFOPjDk)
