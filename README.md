# Assignment5_Customizellm

# ğŸ§  LLM Fine-Tuning & Reward Modeling Assignment using Unsloth
---

## ğŸ“ Contents

This repository contains Colab notebooks and demonstration videos for the following tasks using the **Unsloth AI** library:

- A) Fine-tuning various open-weight LLMs on unique use cases  
- B) Continued Pretraining (Language Learning)  
- C) Chat Templates for multiple tasks  
- D) Reward Modeling using ORPO & DPO  
- E) Continued Fine-Tuning from Custom Checkpoints  
- F) Mental Health Chatbot using Phi-3  
- G) Ollama Export + Inference  
---

## ğŸ§© A) Fine-Tuning LLMs on Unique Use Cases

| Model Used       | Use Case            | Colab Notebook | Video Link |
|------------------|---------------------|----------------|------------|
| LLaMA 3.1 (8B)   | Coding Assistant    | [Colab]()     | [Video](https://youtu.be/g1GIxGEVRa0) |
| Mistral NeMo     | Conversational chat  | [Colab](https://colab.research.google.com/drive/1FPYlz8vdaGCHA-6vipkUzJDZQB9Bxe17#scrollTo=05ae6956)     | [Video](https://youtu.be/o-4VjLK0AFw) |
| Gemma 2 (9B)     | Text Summarization   | [Colab](https://colab.research.google.com/drive/1X3nYiYiZjdCMgBQ7SJGCaZeLVUqOwtz0)     | [Video](https://youtu.be/Lr6QMYuhTSI) |
| Phi3.5           | Sentiment Analysis   | [Colab](https://colab.research.google.com/drive/181ccUJMnpajYeQLkEpnB_ZSSfotSEm0A)     | [Video](#) |

> *Note: Each notebook uses LoRA + Unsloth-based fine-tuning and logs loss, metrics, and inference validation.*

---

##  B) Continued Pretraining

- ğŸ§¾ **Task:** Make LLM learn a new language  
- ğŸ“˜ **Language Used:** [Telugu]  
- ğŸ”— [Colab Notebook]()  
- ğŸ¥ [Video Walkthrough](#)

---

##  C) Chat Templates Use Cases

| Task Type                      | Colab Notebook | Video Link |
|-------------------------------|----------------|------------|
| Conversational Chatbot        | [Colab](#)     | [Video](#) |
| Classification via Templates  | [Colab](#)     | [Video](#) |
| Extended Context (TinyLlama)  | [Colab](#)     | [Video](#) |
| Multi-Dataset Single Training | [Colab](#)     | [Video](#) |

---

## ğŸ¯ D) Reward Modeling - DPO & ORPO

| Reward Method | Colab Notebook | Video Link |
|---------------|----------------|------------|
| DPO (Direct Preference) | [Colab](#) | [Video](#) |
| ORPO (Optimal Reward)   | [Colab](#) | [Video](#) |

> *Each notebook uses preference-based data and logs accuracy/reward shift.*

---

## ğŸ§© E) Continued Fine-tuning from Custom Checkpoint

- ğŸ”— [Colab Notebook](#)  
- ğŸ¥ [Video Walkthrough](#)

---

## ğŸ§˜â€â™€ï¸ F) Mental Health Chatbot using Phi-3

- ğŸ”— [Colab Notebook](#)  
- ğŸ¥ [Video Walkthrough](#)

> *Model is trained on mental health-specific conversations and validated via chat.*

---

## ğŸ” G) Export to Ollama & Inference

- ğŸ”— [Colab Notebook](#)  
- ğŸ”— [Model exported to Ollama](#)  
- ğŸ§ª **Local Inference Demo**: [Video Link](#)
