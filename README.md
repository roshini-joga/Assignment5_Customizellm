# Assignment5_Customizellm

# 🧠 LLM Fine-Tuning & Reward Modeling Assignment using Unsloth

## 🔗 Submission by: [Your Name]  
## 📅 Date: [Submission Date]

---

## 📁 Contents

This repository contains Colab notebooks and demonstration videos for the following tasks using the **Unsloth AI** library:

- A) Fine-tuning various open-weight LLMs on unique use cases  
- B) Continued Pretraining (Language Learning)  
- C) Chat Templates for multiple tasks  
- D) Reward Modeling using ORPO & DPO  
- E) Continued Fine-Tuning from Custom Checkpoints  
- F) Mental Health Chatbot using Phi-3  
- G) Ollama Export + Inference  
- ✅ GitHub Repo with documentation (this file)

---

## ✅ Submission Guidelines

Each section contains:
- 🔗 A **Google Colab notebook link** (with outputs)
- 🎥 A **video walkthrough link** (explaining the process and results)

---

## 🧩 A) Fine-Tuning LLMs on Unique Use Cases

| Model Used       | Use Case            | Colab Notebook | Video Link |
|------------------|---------------------|----------------|------------|
| LLaMA 3.1 (8B)    | Chatbot              | [Colab](#)     | [Video](#) |
| Mistral NeMo     | Coding Assistant     | [Colab](#)     | [Video](#) |
| Gemma 2 (9B)     | Text Classification  | [Colab](#)     | [Video](#) |
| TinyLlama        | Long Context Chat    | [Colab](#)     | [Video](#) |

> *Note: Each notebook uses LoRA + Unsloth-based fine-tuning and logs loss, metrics, and inference validation.*

---

## 🧠 B) Continued Pretraining

- 🧾 **Task:** Make LLM learn a new language  
- 📘 **Language Used:** [Specify Language]  
- 🔗 [Colab Notebook](#)  
- 🎥 [Video Walkthrough](#)

---

## 💬 C) Chat Templates Use Cases

| Task Type                      | Colab Notebook | Video Link |
|-------------------------------|----------------|------------|
| Conversational Chatbot        | [Colab](#)     | [Video](#) |
| Classification via Templates  | [Colab](#)     | [Video](#) |
| Extended Context (TinyLlama)  | [Colab](#)     | [Video](#) |
| Multi-Dataset Single Training | [Colab](#)     | [Video](#) |

---

## 🎯 D) Reward Modeling - DPO & ORPO

| Reward Method | Colab Notebook | Video Link |
|---------------|----------------|------------|
| DPO (Direct Preference) | [Colab](#) | [Video](#) |
| ORPO (Optimal Reward)   | [Colab](#) | [Video](#) |

> *Each notebook uses preference-based data and logs accuracy/reward shift.*

---

## 🧩 E) Continued Fine-tuning from Custom Checkpoint

- 🔗 [Colab Notebook](#)  
- 🎥 [Video Walkthrough](#)

---

## 🧘‍♀️ F) Mental Health Chatbot using Phi-3

- 🔗 [Colab Notebook](#)  
- 🎥 [Video Walkthrough](#)

> *Model is trained on mental health-specific conversations and validated via chat.*

---

## 🔁 G) Export to Ollama & Inference

- 🔗 [Colab Notebook](#)  
- 🔗 [Model exported to Ollama](#)  
- 🧪 **Local Inference Demo**: [Video Link](#)

---

## 📘 GitHub Repo Structure

